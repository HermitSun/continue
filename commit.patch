diff --git a/continue.patch b/continue.patch
new file mode 100644
index 000000000..17329c02c
--- /dev/null
+++ b/continue.patch
@@ -0,0 +1,304 @@
+diff --git a/core/autocomplete/completionProvider.ts b/core/autocomplete/completionProvider.ts
+index da11f1ed6..b694f0eeb 100644
+--- a/core/autocomplete/completionProvider.ts
++++ b/core/autocomplete/completionProvider.ts
+@@ -251,7 +251,6 @@ export class CompletionProvider {
+     token: AbortSignal | undefined,
+     selectedModelTitle: string | undefined,
+   ): Promise<AutocompleteOutcome | undefined> {
+-    const startTime = Date.now();
+     try {
+       // Debounce
+       const uuid = uuidv4();
+@@ -356,13 +355,6 @@ export class CompletionProvider {
+ 
+       const outcome = await this.getTabCompletion(token, options, llm, input,selectedModelTitle);
+ 
+-      const time = Date.now() - startTime;
+-      // console.log()
+-      // await this.configHandler.logMessage(
+-      //   "Document Path: /continue/core/autocomplete/completionProvider.ts\n"+
+-      //   "provideInlineCompletionItems - time："+time/1000+"s\n"
+-      // );
+-      // "provideInlineCompletionItems 补全结果："+outcome?.completion+"\n"
+       if (!outcome?.completion) {
+         return undefined;
+       }
+@@ -669,11 +661,6 @@ export class CompletionProvider {
+       }
+ 
+       completion = processedCompletion
+-      // await this.configHandler.logMessage(
+-      //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/autocomplete/completionProvider.ts\n"+
+-      //   "使用缓存：getTabCompletion-cachedCompletion\n"+
+-      //   completion+"\n"
+-      // );
+     } else {
+       const stop = [
+         ...(completionOptions?.stop || []),
+@@ -726,13 +713,6 @@ export class CompletionProvider {
+         this.configHandler
+       );
+ 
+-
+-      // const streamCompleteGenerator = llm.streamComplete(prompt, {
+-      //   ...completionOptions,
+-      //   raw: true,
+-      //   stop,
+-      // });
+-
+       // Full stop means to stop the LLM's generation, instead of just truncating the displayed completion
+       const fullStop = () =>
+         this.generatorReuseManager.currentGenerator?.cancel();
+@@ -791,24 +771,9 @@ export class CompletionProvider {
+ 
+ 
+       try {
+-        // await this.configHandler.logMessage(
+-        //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/autocomplete/completionProvider.ts\n"+
+-        //   "步骤1-不使用缓存：getTabCompletion生成\n"
+-        // );
+-        
+         for await (const update of finalGenerator) {
+           completion += update;
+-          // await this.configHandler.logMessage(
+-          //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/autocomplete/completionProvider.ts\n"+
+-          //   "步骤1-不使用缓存：getTabCompletion生成中。。。。\n"+
+-          //   completion +"\n"
+-          // );
+         }
+-        // await this.configHandler.logMessage(
+-        //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/autocomplete/completionProvider.ts\n"+
+-        //   "不使用缓存：finalGenerator\n"+
+-        //   completion+"\n"
+-        // );
+       } catch (e: any) {
+         if (ERRORS_TO_IGNORE.some((err) => e.includes(err))) {
+           return undefined;
+@@ -828,12 +793,6 @@ export class CompletionProvider {
+         llm,
+         configHandler: this.configHandler
+       });
+-      
+-      // await this.configHandler.logMessage(
+-      //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/autocomplete/completionProvider.ts\n"+
+-      //   "步骤1-不使用缓存-后处理：getTabCompletion-processedCompletion\n"+
+-      //   processedCompletion+"\n"
+-      // );
+ 
+       if (!processedCompletion) {
+         return undefined;
+@@ -842,6 +801,14 @@ export class CompletionProvider {
+     }
+ 
+     const time = Date.now() - startTime;
++
++    await this.configHandler.logMessage(
++      "core/autocomplete/completionProvider.ts\n"
++      + "getTabCompletion - time:" + time/1000 +"s\n"
++      + "getTabCompletion - completion: " + completion +"\n"
++      + "getTabCompletion - cacheHit:" + cacheHit +"\n"
++    );
++
+     const timestamp = Date.now();
+     return {
+       time,
+diff --git a/core/autocomplete/postprocessing.ts b/core/autocomplete/postprocessing.ts
+index 32f83ad48..f790e019e 100644
+--- a/core/autocomplete/postprocessing.ts
++++ b/core/autocomplete/postprocessing.ts
+@@ -59,8 +59,9 @@ export function postprocessCompletion({
+   // Don't return empty
+   if (completion.trim().length <= 0) {
+     configHandler.logMessage(
+-      // "Document Path: /ai4math/users/xmlu/continue_env/continue/core/autocomplete/postprocessing.ts\n"
+-      "后处理：补全结果为空，返回 undefined\n"
++      "autocomplete/postprocessing.ts\n" 
++      + "completion: "+completion+"\n"
++      + "后处理：补全结果为空，返回 undefined\n"
+     )
+     return undefined;
+   }
+@@ -72,21 +73,23 @@ export function postprocessCompletion({
+       .filter((line) => line.trim().length > 0)
+       .slice(1)  // 获取第二个及以后的非空行
+       .join("\n");  // 将数组转换为字符串，以换行符连接
+-    // configHandler.logMessage(
+-    //   // "Document Path: /ai4math/users/xmlu/continue_env/continue/core/autocomplete/postprocessing.ts\n"
+-    //   + "后处理：如果只是重复上面的一行，返回 secondLineAndAfterOfCompletion："+secondLineAndAfterOfCompletion+"\n"
+-    //   + "completion: "+completion+"\n"
+-    // )
++    
++    configHandler.logMessage(
++      "autocomplete/postprocessing.ts\n"
++      + "completion: "+completion+"\n"
++      + "后处理：如果只是重复上面的一行，返回 secondLineAndAfterOfCompletion："+secondLineAndAfterOfCompletion+"\n"
++    )
+     if (secondLineAndAfterOfCompletion == undefined) return undefined;
+     else completion = secondLineAndAfterOfCompletion;
+   }
+ 
+   // Filter out repetitions of many lines in a row
+   if (isExtremeRepetition(completion)) {
+-    // configHandler.logMessage(
+-    //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/autocomplete/postprocessing.ts\n"
+-    //   + "后处理：连续多行的重复内容，返回 undefined\n"
+-    // )
++    configHandler.logMessage(
++      "autocomplete/postprocessing.ts\n"
++      + "completion: "+completion+"\n"
++      + "后处理：连续多行的重复内容，返回 undefined\n"
++    )
+     return undefined;
+   }
+ 
+@@ -111,11 +114,12 @@ export function postprocessCompletion({
+     !(prefix.endsWith("\n") ||prefix.endsWith("\t")||prefix.endsWith("  ")) &&
+     (suffix.startsWith("\n") || suffix.trim().length === 0)
+   ) {
+-    // configHandler.logMessage(
+-    //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/autocomplete/postprocessing.ts\n"
+-    //   + "后处理：如果补全以多个空格开始，但光标位于行尾, 那么可能应该换行，返回undefined\n"
+-    //   + "前缀以" + prefix.charCodeAt(prefix.length - 1) + "结束\n"
+-    // )
++    configHandler.logMessage(
++      "core/autocomplete/postprocessing.ts\n"
++      + "completion: "+completion+"\n"
++      + "后处理：如果补全以多个空格开始，但光标位于行尾, 那么可能应该换行，返回undefined\n"
++      + "前缀以" + prefix.charCodeAt(prefix.length - 1) + "结束\n"
++    )
+     // completion = "\n" + completion;
+     return undefined;
+   }
+diff --git a/core/llm/index.ts b/core/llm/index.ts
+index ea0580f95..e460a192d 100644
+--- a/core/llm/index.ts
++++ b/core/llm/index.ts
+@@ -465,13 +465,19 @@ export abstract class BaseLLM implements ILLM {
+       suffix,
+       completionOptions,
+     )) {
+-      // 新增
+-      // 如果是连续空行不返回
+-      if (chunk.trim() === "" && completion[completion.length - 1] === "\n") {
+-        continue;
++      let newChunk = chunk;
++      // 新增后处理
++      // 如果第一行为空行，不返回
++      if (completion.length === 0 || completion[completion.length - 1] === "\n") {
++        while (newChunk.startsWith("\n")) {
++          newChunk = newChunk.slice(1);
++        }
++        if (newChunk.length === 0) {
++            continue;
++        }
+       }
+-      completion += chunk;
+-      yield chunk;
++      completion += newChunk;
++      yield newChunk;
+     }
+ 
+     this._logTokensGenerated(
+@@ -485,13 +491,10 @@ export abstract class BaseLLM implements ILLM {
+     const lastNonEmptyLine = prefixLines[prefixLines.length - 1];
+ 
+     if (log && this.writeLog) {
+-      // await this.writeLog(
+-      //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/llm/index.ts\n"
+-      //   + "streamFim - time："+time/1000+"s\n"
+-      //   + "streamFim - completion: \n"+completion+"\n"
+-      // );
+       await this.writeLog(
+-        "streamFim - completion: \n"+completion+"\n"
++        "core/llm/index.ts\n"
++        + "streamFim - time: "+time/1000+"s\n"
++        + "streamFim - completion: \n"+completion+"\n"
+       );
+     }
+     return {
+@@ -537,20 +540,28 @@ export abstract class BaseLLM implements ILLM {
+ 
+     let completion = "";
+     for await (const chunk of this._streamComplete(prompt, completionOptions)) {
+-      completion += chunk;
+-      yield chunk;
++      let newChunk = chunk;
++      // 新增后处理
++      // 如果第一行为空行，不返回
++      if (completion.length === 0 || completion[completion.length - 1] === "\n") {
++        while (newChunk.startsWith("\n")) {
++          newChunk = newChunk.slice(1);
++        }
++        if (newChunk.length === 0) {
++            continue;
++        }
++      }
++      completion += newChunk;
++      yield newChunk;
+     }
+ 
+     this._logTokensGenerated(completionOptions.model, prompt, completion);
+     const time = Date.now() - startTime;
+     if (log && this.writeLog) {
+-      // await this.writeLog(
+-      //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/llm/index.ts\n"+
+-      //   "streamComplete - time: "+time/1000+"s\n"
+-      //   + "streamComplete - completion: \n"+completion+"\n"
+-      // );
+       await this.writeLog(
+-        "streamComplete - completion: \n"+completion+"\n"
++        "core/llm/index.ts\n"
++        + "streamComplete - time: "+time/1000+"s\n"
++        + "streamComplete - completion: \n"+completion+"\n"
+       );
+     }
+ 
+@@ -595,13 +606,10 @@ export abstract class BaseLLM implements ILLM {
+ 
+     const time = Date.now() - startTime;
+     if (log && this.writeLog) {
+-      // await this.writeLog(
+-      //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/llm/index.ts\n"+
+-      //   "complete - time: "+time/1000+"s\n"
+-      //   +"complete - completion: \n"+completion+"\n"
+-      // );
+       await this.writeLog(
+-        "complete - completion: \n"+completion+"\n"
++        "core/llm/index.ts\n"
++        + "complete - time: "+time/1000+"s\n"
++        + "complete - completion: \n"+completion+"\n"
+       );
+     }
+ 
+@@ -670,13 +678,10 @@ export abstract class BaseLLM implements ILLM {
+ 
+     const time = Date.now() - startTime;
+     if (log && this.writeLog) {
+-      // await this.writeLog(
+-      //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/llm/index.ts\n"+
+-      //   "streamChat - time: "+time/1000 + "s\n"+
+-      //   "streamChat - completion: \n"+completion+"\n"
+-      // );
+       await this.writeLog(
+-        "streamChat - completion: \n"+completion+"\n"
++        "core/llm/index.ts\n"
++        + "streamChat - time: "+time/1000+"s\n"
++        + "streamChat - completion: \n"+completion+"\n"
+       );
+     }
+ 
+diff --git a/extensions/vscode/src/autocomplete/completionProvider.ts b/extensions/vscode/src/autocomplete/completionProvider.ts
+index ade2a16e8..daa4c3275 100644
+--- a/extensions/vscode/src/autocomplete/completionProvider.ts
++++ b/extensions/vscode/src/autocomplete/completionProvider.ts
+@@ -220,7 +220,8 @@ export class ContinueCompletionProvider
+         );
+ 
+       const time = Date.now() - startTime;
+-      await console.log(
++      await this.configHandler.logMessage(
++        "extensions/vscode/src/autocomplete/completionProvider.ts\n"+ 
+         `ContinueCompletionProvider - time：`+time/1000 + `s\n`+
+         `ContinueCompletionProvider - completion：`+outcome?.completion + `\n`
+       );
diff --git a/core/autocomplete/completionProvider.ts b/core/autocomplete/completionProvider.ts
index da11f1ed6..0377e3198 100644
--- a/core/autocomplete/completionProvider.ts
+++ b/core/autocomplete/completionProvider.ts
@@ -152,7 +152,7 @@ export class CompletionProvider {
   private static debounceTimeout: NodeJS.Timeout | undefined = undefined;
   private static debouncing = false;
   private static lastUUID: string | undefined = undefined;
-
+  private times: number[];
   constructor(
     private readonly configHandler: ConfigHandler,
     private readonly ide: IDE,
@@ -168,6 +168,7 @@ export class CompletionProvider {
       this.importDefinitionsService,
       this.ide,
     );
+    this.times = [];
   }
 
   private importDefinitionsService: ImportDefinitionsService;
@@ -251,7 +252,6 @@ export class CompletionProvider {
     token: AbortSignal | undefined,
     selectedModelTitle: string | undefined,
   ): Promise<AutocompleteOutcome | undefined> {
-    const startTime = Date.now();
     try {
       // Debounce
       const uuid = uuidv4();
@@ -356,13 +356,6 @@ export class CompletionProvider {
 
       const outcome = await this.getTabCompletion(token, options, llm, input,selectedModelTitle);
 
-      const time = Date.now() - startTime;
-      // console.log()
-      // await this.configHandler.logMessage(
-      //   "Document Path: /continue/core/autocomplete/completionProvider.ts\n"+
-      //   "provideInlineCompletionItems - time："+time/1000+"s\n"
-      // );
-      // "provideInlineCompletionItems 补全结果："+outcome?.completion+"\n"
       if (!outcome?.completion) {
         return undefined;
       }
@@ -557,7 +550,10 @@ export class CompletionProvider {
       extrasSnippets = extrasSnippets.filter((snippet) => {
         return workspaceDirs.some((dir) => snippet.filepath.startsWith(dir));
       });
+
     }
+
+
     let { prefix, suffix, completeMultiline, snippets } =
       await constructAutocompletePrompt(
         filepath,
@@ -573,6 +569,7 @@ export class CompletionProvider {
         extrasSnippets,
         this.importDefinitionsService,
         this.rootPathContextService,
+        this.configHandler
       );
     // If prefix is manually passed
     if (manuallyPassPrefix) {
@@ -669,11 +666,6 @@ export class CompletionProvider {
       }
 
       completion = processedCompletion
-      // await this.configHandler.logMessage(
-      //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/autocomplete/completionProvider.ts\n"+
-      //   "使用缓存：getTabCompletion-cachedCompletion\n"+
-      //   completion+"\n"
-      // );
     } else {
       const stop = [
         ...(completionOptions?.stop || []),
@@ -703,8 +695,9 @@ export class CompletionProvider {
       // prefix = CodebaseContext + "```" + filepath + "```" + prefix;
       // const RAGtime = Date.now() - RAGstartTime;
       // await this.configHandler.logMessage(
-      //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/autocomplete/completionProvider.ts\n"+
-      //   "获取codebasecontext耗时："+RAGtime/1000+"s\n"
+      //   "autocomplete/completionProvider.ts\n" +
+      //   "CodebaseContext - time: " + RAGtime/1000 + "s\n" +
+      //   "CodebaseContext - CodebaseContext: " + CodebaseContext + "\n"
       // );
 
 
@@ -726,13 +719,6 @@ export class CompletionProvider {
         this.configHandler
       );
 
-
-      // const streamCompleteGenerator = llm.streamComplete(prompt, {
-      //   ...completionOptions,
-      //   raw: true,
-      //   stop,
-      // });
-
       // Full stop means to stop the LLM's generation, instead of just truncating the displayed completion
       const fullStop = () =>
         this.generatorReuseManager.currentGenerator?.cancel();
@@ -791,24 +777,9 @@ export class CompletionProvider {
 
 
       try {
-        // await this.configHandler.logMessage(
-        //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/autocomplete/completionProvider.ts\n"+
-        //   "步骤1-不使用缓存：getTabCompletion生成\n"
-        // );
-        
         for await (const update of finalGenerator) {
           completion += update;
-          // await this.configHandler.logMessage(
-          //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/autocomplete/completionProvider.ts\n"+
-          //   "步骤1-不使用缓存：getTabCompletion生成中。。。。\n"+
-          //   completion +"\n"
-          // );
         }
-        // await this.configHandler.logMessage(
-        //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/autocomplete/completionProvider.ts\n"+
-        //   "不使用缓存：finalGenerator\n"+
-        //   completion+"\n"
-        // );
       } catch (e: any) {
         if (ERRORS_TO_IGNORE.some((err) => e.includes(err))) {
           return undefined;
@@ -828,12 +799,6 @@ export class CompletionProvider {
         llm,
         configHandler: this.configHandler
       });
-      
-      // await this.configHandler.logMessage(
-      //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/autocomplete/completionProvider.ts\n"+
-      //   "步骤1-不使用缓存-后处理：getTabCompletion-processedCompletion\n"+
-      //   processedCompletion+"\n"
-      // );
 
       if (!processedCompletion) {
         return undefined;
@@ -842,6 +807,28 @@ export class CompletionProvider {
     }
 
     const time = Date.now() - startTime;
+    this.times.push(time);
+    const meanTime = this.times.reduce((acc, t) => acc + t, 0) / this.times.length;
+    const sortedTimes = [...this.times].sort((a, b) => a - b);
+    const mid = Math.floor(sortedTimes.length / 2);
+    const medianTime = sortedTimes.length % 2 !== 0
+        ? sortedTimes[mid]
+        : (sortedTimes[mid - 1] + sortedTimes[mid]) / 2;
+    const p95Time = sortedTimes[Math.ceil(0.95 * sortedTimes.length) - 1];
+    const p99Time = sortedTimes[Math.ceil(0.99 * sortedTimes.length) - 1];
+    await this.configHandler.logMessage(
+      "core/autocomplete/completionProvider.ts\n" + 
+      "getTabCompletion - Time: " + time/1000 + "s\n" + 
+      "getTabCompletion - Completion: " + completion + "\n" + 
+      "getTabCompletion - cacheHit: " + cacheHit + "\n" + 
+      "getTabCompletion - Data Count: " + this.times.length + "\n" + 
+      "getTabCompletion - Mean Time: " + (meanTime / 1000) + "s\n" +
+      "getTabCompletion - Median Time: " + (medianTime / 1000) + "s\n" +
+      "getTabCompletion - P95 Time: " + (p95Time / 1000) + "s\n" +
+      "getTabCompletion - P99 Time: " + (p99Time / 1000) + "s\n"
+    );
+    
+
     const timestamp = Date.now();
     return {
       time,
@@ -876,7 +863,6 @@ export class CompletionProvider {
     contextItems.push(...codebaseItems);
     for (const codebaseItem of codebaseItems) {
       contextItemsText += codebaseItem.content + "\n\n";
-      
     }
 
     // await this.configHandler.logMessage(
diff --git a/core/autocomplete/constructPrompt.ts b/core/autocomplete/constructPrompt.ts
index 9e9f430b9..b3d71675b 100644
--- a/core/autocomplete/constructPrompt.ts
+++ b/core/autocomplete/constructPrompt.ts
@@ -23,6 +23,7 @@ import { RecentlyEditedRange, findMatchingRange } from "./recentlyEdited.js";
 import { ImportDefinitionsService } from "./services/ImportDefinitionsService.js";
 import { RootPathContextService } from "./services/RootPathContextService.js";
 import { shouldCompleteMultiline } from "./shouldCompleteMultiline.js";
+import { ConfigHandler } from "../config/ConfigHandler.js";
 
 export function languageForFilepath(
   filepath: string,
@@ -44,6 +45,7 @@ export async function constructAutocompletePrompt(
   extraSnippets: AutocompleteSnippet[],
   importDefinitionsService: ImportDefinitionsService,
   rootPathContextService: RootPathContextService,
+  configHandler: ConfigHandler 
 ): Promise<{
   prefix: string;
   suffix: string;
@@ -51,10 +53,24 @@ export async function constructAutocompletePrompt(
   completeMultiline: boolean;
   snippets: AutocompleteSnippet[];
 }> {
+
+  // await configHandler.logMessage(
+  //   "core/autocomplete/constructPrompt.ts\n" +
+  //   "filepath: " + filepath + "\n" +
+  //   "cursorLine: " + cursorLine + "\n" +
+  //   "fullPrefix: " + fullPrefix + "\n" + 
+  //   "fullSuffix: " + fullSuffix + "\n" +
+  //   "language: " + JSON.stringify({...language},null,2) + "\n" +
+  //   "recentlyEditedRanges: " + JSON.stringify({...recentlyEditedRanges},null,2) + "\n" +
+  //   "modelName: " + modelName + "\n" +
+  //   "extraSnippets: " + JSON.stringify({...extraSnippets},null,2) + "\n" +
+  //   "importDefinitionsService: " + importDefinitionsService + "\n"
+  // );
+  
   // Construct basic prefix
   const maxPrefixTokens = options.maxPromptTokens * options.prefixPercentage;
   const prefix = pruneLinesFromTop(fullPrefix, maxPrefixTokens, modelName);
-
+  
   // Construct suffix
   const maxSuffixTokens = Math.min(
     options.maxPromptTokens - countTokens(prefix, modelName),
@@ -75,10 +91,8 @@ export async function constructAutocompletePrompt(
 
   // Find external snippets
   let snippets: AutocompleteSnippet[] = [];
- 
   if (options.useOtherFiles) {
     snippets.push(...extraSnippets);
-                                 
     const windowAroundCursor =
       fullPrefix.slice(
         -options.slidingWindowSize * options.slidingWindowPrefixPercentage,
diff --git a/core/autocomplete/postprocessing.ts b/core/autocomplete/postprocessing.ts
index 32f83ad48..9532cb01a 100644
--- a/core/autocomplete/postprocessing.ts
+++ b/core/autocomplete/postprocessing.ts
@@ -59,8 +59,9 @@ export function postprocessCompletion({
   // Don't return empty
   if (completion.trim().length <= 0) {
     configHandler.logMessage(
-      // "Document Path: /ai4math/users/xmlu/continue_env/continue/core/autocomplete/postprocessing.ts\n"
-      "后处理：补全结果为空，返回 undefined\n"
+      "autocomplete/postprocessing.ts\n" 
+      + "completion: "+completion+"\n"
+      + "后处理：补全结果为空，返回 undefined\n"
     )
     return undefined;
   }
@@ -72,21 +73,23 @@ export function postprocessCompletion({
       .filter((line) => line.trim().length > 0)
       .slice(1)  // 获取第二个及以后的非空行
       .join("\n");  // 将数组转换为字符串，以换行符连接
-    // configHandler.logMessage(
-    //   // "Document Path: /ai4math/users/xmlu/continue_env/continue/core/autocomplete/postprocessing.ts\n"
-    //   + "后处理：如果只是重复上面的一行，返回 secondLineAndAfterOfCompletion："+secondLineAndAfterOfCompletion+"\n"
-    //   + "completion: "+completion+"\n"
-    // )
+    
+    configHandler.logMessage(
+      "autocomplete/postprocessing.ts\n"
+      + "completion: "+completion+"\n"
+      + "后处理：如果只是重复上面的一行，返回 secondLineAndAfterOfCompletion："+secondLineAndAfterOfCompletion+"\n"
+    )
     if (secondLineAndAfterOfCompletion == undefined) return undefined;
     else completion = secondLineAndAfterOfCompletion;
   }
 
   // Filter out repetitions of many lines in a row
   if (isExtremeRepetition(completion)) {
-    // configHandler.logMessage(
-    //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/autocomplete/postprocessing.ts\n"
-    //   + "后处理：连续多行的重复内容，返回 undefined\n"
-    // )
+    configHandler.logMessage(
+      "autocomplete/postprocessing.ts\n"
+      + "completion: "+completion+"\n"
+      + "后处理：连续多行的重复内容，返回 undefined\n"
+    )
     return undefined;
   }
 
@@ -111,11 +114,12 @@ export function postprocessCompletion({
     !(prefix.endsWith("\n") ||prefix.endsWith("\t")||prefix.endsWith("  ")) &&
     (suffix.startsWith("\n") || suffix.trim().length === 0)
   ) {
-    // configHandler.logMessage(
-    //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/autocomplete/postprocessing.ts\n"
-    //   + "后处理：如果补全以多个空格开始，但光标位于行尾, 那么可能应该换行，返回undefined\n"
-    //   + "前缀以" + prefix.charCodeAt(prefix.length - 1) + "结束\n"
-    // )
+    configHandler.logMessage(
+      "core/autocomplete/postprocessing.ts\n"
+      + "completion: "+completion+"\n"
+      + "后处理：如果补全以多个空格开始，但光标位于行尾, 那么可能应该换行，返回undefined\n"
+      + "前缀以ASCII码" + prefix.charCodeAt(prefix.length - 1) + "结束\n"
+    )
     // completion = "\n" + completion;
     return undefined;
   }
diff --git a/core/config/load.ts b/core/config/load.ts
index 795b62668..312b2ce9a 100644
--- a/core/config/load.ts
+++ b/core/config/load.ts
@@ -242,15 +242,6 @@ async function intermediateToFinalConfig(
 ): Promise<ContinueConfig> {
   // Auto-detect models
   let models: BaseLLM[] = [];
-
-  // writeLog(
-  //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/config/load.ts\n"+
-  //   "identify: intermediateToFinalConfig-config\n"+
-  //   JSON.stringify({
-  //     ...config
-  //   },null,2,)
-  // )
-  
   for (const desc of config.models) {
     if (isModelDescription(desc)) {
       const llm = await llmFromDescription(
@@ -262,6 +253,7 @@ async function intermediateToFinalConfig(
         config.completionOptions,
         config.systemMessage,
       );
+
       if (!llm) {
         continue;
       }
diff --git a/core/context/retrieval/pipelines/RerankerRetrievalPipeline.ts b/core/context/retrieval/pipelines/RerankerRetrievalPipeline.ts
index 50e4b0e46..505724cba 100644
--- a/core/context/retrieval/pipelines/RerankerRetrievalPipeline.ts
+++ b/core/context/retrieval/pipelines/RerankerRetrievalPipeline.ts
@@ -27,7 +27,7 @@ export default class RerankerRetrievalPipeline extends BaseRetrievalPipeline {
     // );
 
     // const recentlyEditedFilesstartTime = Date.now();
-    const recentlyEditedFilesChunks = await this.retrieveAndChunkRecentlyEditedFiles(nRetrieve);
+    // const recentlyEditedFilesChunks = await this.retrieveAndChunkRecentlyEditedFiles(nRetrieve);
     // const recentlyEditedFilestime = Date.now() - recentlyEditedFilesstartTime;
     // this.writeLog(
     //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/context/retrieval/pipelines/RerankerRetrievalPipeline.ts\n"+
@@ -49,7 +49,7 @@ export default class RerankerRetrievalPipeline extends BaseRetrievalPipeline {
     // );
 
     retrievalResults.push(
-      ...recentlyEditedFilesChunks,
+      // ...recentlyEditedFilesChunks,
       ...ftsChunks,
       ...embeddingsChunks,
       ...repoMapChunks,
diff --git a/core/core.ts b/core/core.ts
index e09d69550..2021d567b 100644
--- a/core/core.ts
+++ b/core/core.ts
@@ -316,11 +316,6 @@ export class Core {
       if (!provider) {
         return [];
       }
-      // this.configHandler.logMessage(
-      //   "文件路径: /ai4math/users/xmlu/continue_env/continue/core/core.ts\n"+
-      //   "context/getContextItems\n"+
-      //   "msg: " + JSON.stringify({...msg},null,2)+"\n"
-      // )
       try {
         const id: ContextItemId = {
           providerTitle: provider.description.title,
@@ -372,10 +367,6 @@ export class Core {
       abortedMessageIds: Set<string>,
       msg: Message<ToCoreProtocol["llm/streamChat"][0]>,
     ) {
-      // configHandler.logMessage(
-      //   "文件路径: /ai4math/users/xmlu/continue_env/continue/core/core.ts\n"+
-      //   "llm/llmStreamChat\n")
-
       const config = await configHandler.loadConfig();
 
       // Stop TTS on new StreamChat
@@ -422,9 +413,6 @@ export class Core {
       abortedMessageIds: Set<string>,
       msg: Message<ToCoreProtocol["llm/streamComplete"][0]>,
     ) {
-      // configHandler.logMessage(
-      //   "文件路径: /ai4math/users/xmlu/continue_env/continue/core/core.ts\n"+
-      //   "llm/streamComplete\n")
       const model = await configHandler.llmFromTitle(msg.data.title);
       const gen = model.streamComplete(
         msg.data.prompt,
@@ -581,9 +569,6 @@ export class Core {
 
     // Autocomplete
     on("autocomplete/complete", async (msg) => {
-      // this.configHandler.logMessage(
-      //   "文件路径: /ai4math/users/xmlu/continue_env/continue/core/core.ts\n"+
-      //   "autocomplete/complete\n")
       const outcome =
         await this.completionProvider.provideInlineCompletionItems(
           msg.data,
@@ -604,9 +589,6 @@ export class Core {
       abortedMessageIds: Set<string>,
       msg: Message<ToCoreProtocol["streamDiffLines"][0]>,
     ) {
-      // configHandler.logMessage(
-      //   "文件路径: /ai4math/users/xmlu/continue_env/continue/core/core.ts\n"+
-      //   "streamDiffLines\n")
       const data = msg.data;
       const llm = await configHandler.llmFromTitle(msg.data.modelTitle);
       for await (const diffLine of streamDiffLines(
diff --git a/core/edit/streamDiffLines.ts b/core/edit/streamDiffLines.ts
index cad33b3a2..7d7d1bab7 100644
--- a/core/edit/streamDiffLines.ts
+++ b/core/edit/streamDiffLines.ts
@@ -68,14 +68,12 @@ export async function* streamDiffLines(
     },
     true,
   );
-
   // Strip common indentation for the LLM, then add back after generation
   let oldLines =
     highlighted.length > 0
       ? highlighted.split("\n")
       : // When highlighted is empty, we need to combine last line of prefix and first line of suffix to determine the line being edited
         [(prefix + suffix).split("\n")[prefix.split("\n").length - 1]];
-
   // But if that line is empty, we can assume we are insertion-only
   if (oldLines.length === 1 && oldLines[0].trim() === "") {
     oldLines = [];
@@ -83,7 +81,6 @@ export async function* streamDiffLines(
 
   // Trim end of oldLines, otherwise we have trailing \r on every line for CRLF files
   oldLines = oldLines.map((line) => line.trimEnd());
-
   const prompt = constructPrompt(
     prefix,
     highlighted,
@@ -92,8 +89,14 @@ export async function* streamDiffLines(
     input,
     language,
   );
-  const inept = modelIsInept(llm.model);
 
+  /** DEBUG 
+   */
+  let llm_model = llm.model;
+  if (llm.model === undefined){
+    llm_model = llm.completionOptions.model;
+  }
+  const inept = modelIsInept(llm_model); // llm.model -> llm_model
   const options: LLMFullCompletionOptions = {};
   const completion =
     typeof prompt === "string"
@@ -101,7 +104,6 @@ export async function* streamDiffLines(
       : llm.streamChat(prompt);
 
   let lines = streamLines(completion);
-
   lines = filterEnglishLinesAtStart(lines);
   lines = filterCodeBlockLines(lines);
   lines = stopAtLines(lines, () => {});
@@ -110,7 +112,6 @@ export async function* streamDiffLines(
     // lines = fixCodeLlamaFirstLineIndentation(lines);
     lines = filterEnglishLinesAtEnd(lines);
   }
-
   let diffLines = streamDiff(oldLines, lines);
   diffLines = filterLeadingAndTrailingNewLineInsertion(diffLines);
   if (highlighted.length === 0) {
@@ -118,7 +119,6 @@ export async function* streamDiffLines(
     const indentation = line.slice(0, line.length - line.trimStart().length);
     diffLines = addIndentation(diffLines, indentation);
   }
-
   let seenGreen = false;
   for await (const diffLine of diffLines) {
     yield diffLine;
diff --git a/core/llm/index.ts b/core/llm/index.ts
index ea0580f95..959f94325 100644
--- a/core/llm/index.ts
+++ b/core/llm/index.ts
@@ -89,7 +89,8 @@ export abstract class BaseLLM implements ILLM {
   
   uniqueId: string;
   model: string;
-
+  times: number[];
+  ttfts: number[];
   title?: string;
   systemMessage?: string;
   contextLength: number;
@@ -121,7 +122,7 @@ export abstract class BaseLLM implements ILLM {
   watsonxStopToken?: string;
   watsonxApiVersion?: string;
   watsonxFullUrl?: string;
-
+  
   private _llmOptions: LLMOptions;
 
   constructor(_options: LLMOptions) {
@@ -161,6 +162,7 @@ export abstract class BaseLLM implements ILLM {
             )
           : DEFAULT_MAX_TOKENS),
     };
+
     if (CompletionOptionsForModels[options.model as ModelName]) {
       this.completionOptions = mergeJson(
         this.completionOptions,
@@ -205,6 +207,9 @@ export abstract class BaseLLM implements ILLM {
     this.apiType = options.apiType;
     this.region = options.region;
     this.projectId = options.projectId;
+    // 新增 times 数组
+    this.times = [];
+    this.ttfts = [];
   }
 
   listModels(): Promise<string[]> {
@@ -465,13 +470,19 @@ export abstract class BaseLLM implements ILLM {
       suffix,
       completionOptions,
     )) {
-      // 新增
-      // 如果是连续空行不返回
-      if (chunk.trim() === "" && completion[completion.length - 1] === "\n") {
-        continue;
+      let newChunk = chunk;
+      // 新增后处理
+      // 如果第一行为空行，不返回
+      if (completion.length === 0 || completion[completion.length - 1] === "\n") {
+        while (newChunk.startsWith("\n")) {
+          newChunk = newChunk.slice(1);
+        }
+        if (newChunk.length === 0) {
+            continue;
+        }
       }
-      completion += chunk;
-      yield chunk;
+      completion += newChunk;
+      yield newChunk;
     }
 
     this._logTokensGenerated(
@@ -485,13 +496,10 @@ export abstract class BaseLLM implements ILLM {
     const lastNonEmptyLine = prefixLines[prefixLines.length - 1];
 
     if (log && this.writeLog) {
-      // await this.writeLog(
-      //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/llm/index.ts\n"
-      //   + "streamFim - time："+time/1000+"s\n"
-      //   + "streamFim - completion: \n"+completion+"\n"
-      // );
       await this.writeLog(
-        "streamFim - completion: \n"+completion+"\n"
+        "core/llm/index.ts\n"
+        + "streamFim - time: "+time/1000+"s\n"
+        + "streamFim - completion: \n"+completion+"\n"
       );
     }
     return {
@@ -536,23 +544,73 @@ export abstract class BaseLLM implements ILLM {
 
 
     let completion = "";
+    let flag = false;
+    
+    const start_TTFT_Time = Date.now(); // 记录开始时间
     for await (const chunk of this._streamComplete(prompt, completionOptions)) {
-      completion += chunk;
-      yield chunk;
+      let newChunk = chunk;
+      // 新增后处理
+      // 如果第一行为空行，不返回
+      if (completion.length === 0 || completion[completion.length - 1] === "\n") {
+        while (newChunk.startsWith("\n")) {
+          newChunk = newChunk.slice(1);
+        }
+        if (newChunk.length === 0) {
+            continue;
+        }
+      }
+
+      // 计算ttft时间
+      if (!flag) {
+        flag = true;
+        const ttfts = Date.now() - start_TTFT_Time; // 计算ttft时间，单位为秒
+        this.ttfts.push(ttfts);
+      }
+    
+      completion += newChunk;
+      yield newChunk;
     }
 
     this._logTokensGenerated(completionOptions.model, prompt, completion);
     const time = Date.now() - startTime;
     if (log && this.writeLog) {
-      // await this.writeLog(
-      //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/llm/index.ts\n"+
-      //   "streamComplete - time: "+time/1000+"s\n"
-      //   + "streamComplete - completion: \n"+completion+"\n"
-      // );
+      this.times.push(time);
+      
+      // 计算时间的统计数据
+      const meanTime = this.times.reduce((acc, t) => acc + t, 0) / this.times.length;
+      const sortedTimes = [...this.times].sort((a, b) => a - b);
+      const mid = Math.floor(sortedTimes.length / 2);
+      const medianTime = sortedTimes.length % 2 !== 0 ? sortedTimes[mid] : (sortedTimes[mid - 1] + sortedTimes[mid]) / 2;
+      const p95Time = sortedTimes[Math.ceil(0.95 * sortedTimes.length) - 1];
+      const p99Time = sortedTimes[Math.ceil(0.99 * sortedTimes.length) - 1];
+  
+      // 计算TTFT的统计数据
+      const meanTTFT = this.ttfts.reduce((acc, t) => acc + t, 0) / this.ttfts.length;
+      const sortedTTFT = [...this.ttfts].sort((a, b) => a - b);
+      const midTTFT = Math.floor(sortedTTFT.length / 2);
+      const medianTTFT = sortedTTFT.length % 2 !== 0 ? sortedTTFT[midTTFT] : (sortedTTFT[midTTFT - 1] + sortedTTFT[midTTFT]) / 2;
+      const p95TTFT = sortedTTFT[Math.ceil(0.95 * sortedTTFT.length) - 1];
+      const p99TTFT = sortedTTFT[Math.ceil(0.99 * sortedTTFT.length) - 1];
+  
       await this.writeLog(
-        "streamComplete - completion: \n"+completion+"\n"
+        "core/llm/index.ts\n" +
+        "streamComplete - Completion: \n" + completion + "\n" +
+        "----------------------Time------------------------\n" +
+        "streamComplete - Time: " + (time / 1000) + "s\n" +
+        "streamComplete - Data Count: " + this.times.length + "\n" + 
+        "streamComplete - Mean Time: " + (meanTime / 1000) + "s\n" +
+        "streamComplete - Median Time: " + (medianTime / 1000) + "s\n" +
+        "streamComplete - P95 Time: " + (p95Time / 1000) + "s\n" + 
+        "streamComplete - P99 Time: " + (p99Time / 1000) + "s\n" +
+        "----------------------Time to First Token------------------------\n" +
+        "streamComplete - TTFT: " + (this.ttfts[this.ttfts.length - 1] / 1000) + "s\n" +
+        "streamComplete - TTFT Count: " + this.ttfts.length + "\n" +
+        "streamComplete - Mean TTFT: " + (meanTTFT / 1000) + "s\n" +
+        "streamComplete - Median TTFT: " + (medianTTFT / 1000) + "s\n" +
+        "streamComplete - P95 TTFT: " + (p95TTFT / 1000) + "s\n" +
+        "streamComplete - P99 TTFT: " + (p99TTFT / 1000) + "s\n"
       );
-    }
+  }
 
     return {
       modelTitle: this.title ?? completionOptions.model,
@@ -595,13 +653,10 @@ export abstract class BaseLLM implements ILLM {
 
     const time = Date.now() - startTime;
     if (log && this.writeLog) {
-      // await this.writeLog(
-      //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/llm/index.ts\n"+
-      //   "complete - time: "+time/1000+"s\n"
-      //   +"complete - completion: \n"+completion+"\n"
-      // );
       await this.writeLog(
-        "complete - completion: \n"+completion+"\n"
+        "core/llm/index.ts\n"
+        + "complete - time: "+time/1000+"s\n"
+        + "complete - completion: \n"+completion+"\n"
       );
     }
 
@@ -670,13 +725,10 @@ export abstract class BaseLLM implements ILLM {
 
     const time = Date.now() - startTime;
     if (log && this.writeLog) {
-      // await this.writeLog(
-      //   "Document Path: /ai4math/users/xmlu/continue_env/continue/core/llm/index.ts\n"+
-      //   "streamChat - time: "+time/1000 + "s\n"+
-      //   "streamChat - completion: \n"+completion+"\n"
-      // );
       await this.writeLog(
-        "streamChat - completion: \n"+completion+"\n"
+        "core/llm/index.ts\n"
+        + "streamChat - time: "+time/1000+"s\n"
+        + "streamChat - completion: \n"+completion+"\n"
       );
     }
 
@@ -765,10 +817,17 @@ export abstract class BaseLLM implements ILLM {
       // Some providers don't allow you to put words in the model's mouth
       // So we have to manually compile the prompt template and use
       // raw /completions, not /chat/completions
+      /** DEBUG: llm 配置格式问题
+       * 读取到的 qwen模型 model 信息不在 this.model，而在 this.completionOptions.model
+       * */ 
+      let this_model = this.model;
+      if (this.model === undefined){
+        this_model = this.completionOptions.model;
+      }
       const templateMessages = autodetectTemplateFunction(
-        this.model,
+        this_model,
         this.providerName,
-        autodetectTemplateType(this.model),
+        autodetectTemplateType(this_model),
       );
       return templateMessages(rendered);
     }
diff --git a/core/llm/llms/OpenAI.ts b/core/llm/llms/OpenAI.ts
index 9362f58b9..8d4c01020 100644
--- a/core/llm/llms/OpenAI.ts
+++ b/core/llm/llms/OpenAI.ts
@@ -194,23 +194,51 @@ class OpenAI extends BaseLLM {
     }
   }
 
+
+
+
   protected async *_legacystreamComplete(
     prompt: string,
     options: CompletionOptions,
   ): AsyncGenerator<string> {
+
     const args: any = this._convertArgs(options, []);
     args.prompt = prompt;
     args.messages = undefined;
 
+  /** DEBUG
+     * 内网 qwen 模型还未开放 completions 接口
+     * edit 功能使用 chat/completions 接口
+     * 自动补全模型 deepseek 使用 completions 接口
+     * */ 
+
+    // let endpoint: "completions" | "chat/completions" = "chat/completions";
+    // if (options.model.includes("deepseek")) {
+    //   endpoint = "completions";
+    // }
+
+    // if (this.writeLog) {
+    //   await this.writeLog(
+    //     "core/llm/llms/OpenAI.ts\n"+
+    //     "_legacystreamComplete - options.model: " + options.model + "\n" +
+    //     "_legacystreamComplete - options: " + JSON.stringify({...options},null,2) + "\n" +
+    //     "_legacystreamComplete - endpoint: " + endpoint + "\n" 
+    //   );
+    // }
+
     const response = await this.fetch(this._getEndpoint("completions"), {
       method: "POST",
       headers: this._getHeaders(),
       body: JSON.stringify({
         ...args,
         stream: true,
-      }),
+      }),   
     });
-
+    // if (this.writeLog) {
+    //   await this.writeLog(
+    //     "_legacystreamComplete - response: " + response + "\n"
+    //   );
+    // }
     for await (const value of streamSse(response)) {
       if (value.choices?.[0]?.text && value.finish_reason !== "eos") {
         yield value.choices[0].text;
@@ -222,6 +250,7 @@ class OpenAI extends BaseLLM {
     messages: ChatMessage[],
     options: CompletionOptions,
   ): AsyncGenerator<ChatMessage> {
+    
     if (
       !CHAT_ONLY_MODELS.includes(options.model) &&
       this.supportsCompletions() &&
@@ -273,12 +302,6 @@ class OpenAI extends BaseLLM {
     options: CompletionOptions,
   ): AsyncGenerator<string> {
     const endpoint = new URL("completions", this.apiBase);
-    // if (this.writeLog){
-    //   await this.writeLog(
-    //     "文件路径：/ai4math/users/xmlu/continue_env/continue/core/llm/llms/OpenAI.ts\n"+
-    //     "OpenAI接口调用"+options.model+"模型进行_streamFim\n"
-    //   )
-    // }
     const prompt="<fim_prefix>"+prefix+"<fim_suffix>"+suffix+"<fim_middle>";
     const resp = await this.fetch(endpoint, {
       method: "POST",
@@ -301,31 +324,24 @@ class OpenAI extends BaseLLM {
       },
     });
     for await (const chunk of streamSse(resp)) {
-      // chunk 格式
-      // chunk: {
-      //   "id": "cmpl-39481d943f0b403f9d1891a6e7dda155",
-      //   "object": "text_completion",
-      //   "created": 1730795368,
-      //   "model": "/models/AI-ModelScope/starcoder2-3b",
-      //   "choices": [
-      //     {
-      //       "index": 0,
-      //       "text": ")\n# merge sort", text就是模型回复的补全结果
-      //       "logprobs": null,
-      //       "finish_reason": "stop",
-      //       "stop_reason": "\ndef"
-      //     }
-      //   ],
-      //   "usage": null
-      // }
-
-
-      // if (this.writeLog){
-      //   await this.writeLog(
-      //     "response: "+JSON.stringify({...resp},null,2)+"\n"
-      //     +"chunk: "+JSON.stringify({...chunk},null,2)+"\n"
-      //   );
-      // }
+      /** DEBUG: api回复格式问题
+       * chunk: {
+       * "id": "cmpl-39481d943f0b403f9d1891a6e7dda155",
+       * "object": "text_completion",
+       * "created": 1730795368,
+       * "model": "/models/AI-ModelScope/starcoder2-3b",
+       * "choices": [
+       *   {
+       *     "index": 0,
+       *     "text": ")\n# merge sort", text就是模型回复的补全结果
+       *     "logprobs": null,
+       *     "finish_reason": "stop",
+       *     "stop_reason": "\ndef"
+       *   }
+       *  ],
+       *  "usage": null
+       * }
+       * */ 
       yield chunk.choices[0].text;
     }
   }
diff --git a/extensions/vscode/continue-0.8.56.vsix b/extensions/vscode/continue-0.8.56.vsix
new file mode 100644
index 000000000..9e2edf200
Binary files /dev/null and b/extensions/vscode/continue-0.8.56.vsix differ
diff --git a/extensions/vscode/package-lock.json b/extensions/vscode/package-lock.json
index fc2eddacd..6845f58fc 100644
--- a/extensions/vscode/package-lock.json
+++ b/extensions/vscode/package-lock.json
@@ -4452,11 +4452,10 @@
     },
     "node_modules/esbuild": {
       "version": "0.17.19",
-      "resolved": "https://registry.npmjs.org/esbuild/-/esbuild-0.17.19.tgz",
+      "resolved": "https://registry.npmmirror.com/esbuild/-/esbuild-0.17.19.tgz",
       "integrity": "sha512-XQ0jAPFkK/u3LcVRcvVHQcTIqD6E2H1fvZMA5dQPSOWb3suUbWbfbRf94pjc0bNzRYLfIrDRQXr7X+LHIm5oHw==",
       "dev": true,
       "hasInstallScript": true,
-      "license": "MIT",
       "bin": {
         "esbuild": "bin/esbuild"
       },
diff --git a/extensions/vscode/src/autocomplete/completionProvider.ts b/extensions/vscode/src/autocomplete/completionProvider.ts
index ade2a16e8..76c5feff1 100644
--- a/extensions/vscode/src/autocomplete/completionProvider.ts
+++ b/extensions/vscode/src/autocomplete/completionProvider.ts
@@ -57,7 +57,7 @@ export class ContinueCompletionProvider
 
   private completionProvider: CompletionProvider;
   private recentlyEditedTracker = new RecentlyEditedTracker();
-
+  private times: number[];
   constructor(
     private readonly configHandler: ConfigHandler,
     private readonly ide: IDE,
@@ -71,7 +71,7 @@ export class ContinueCompletionProvider
       this.onError.bind(this),
       getDefinitionsFromLsp,
     );
-
+    this.times = [];
     vscode.workspace.onDidChangeTextDocument((event) => {
       if (event.document.uri.fsPath === this._lastShownCompletion?.filepath) {
         // console.log("updating completion");
@@ -220,9 +220,24 @@ export class ContinueCompletionProvider
         );
 
       const time = Date.now() - startTime;
-      await console.log(
-        `ContinueCompletionProvider - time：`+time/1000 + `s\n`+
-        `ContinueCompletionProvider - completion：`+outcome?.completion + `\n`
+      this.times.push(time);
+      const meanTime = this.times.reduce((acc, t) => acc + t, 0) / this.times.length;
+      const sortedTimes = [...this.times].sort((a, b) => a - b);
+      const mid = Math.floor(sortedTimes.length / 2);
+      const medianTime = sortedTimes.length % 2 !== 0
+          ? sortedTimes[mid]
+          : (sortedTimes[mid - 1] + sortedTimes[mid]) / 2;
+      const p95Time = sortedTimes[Math.ceil(0.95 * sortedTimes.length) - 1];
+      const p99Time = sortedTimes[Math.ceil(0.99 * sortedTimes.length) - 1];
+      await this.configHandler.logMessage(
+        "extensions/vscode/src/autocomplete/completionProvider.ts\n"+ 
+        "ContinueCompletionProvider - time: " + time/1000 + "s\n"+
+        "ContinueCompletionProvider - completion: " + outcome?.completion + "\n"+
+        "ContinueCompletionProvider - Data Count: " + this.times.length + "\n" + 
+        "ContinueCompletionProvider - Mean Time: " + (meanTime / 1000) + "s\n" +
+        "ContinueCompletionProvider - Median Time: " + (medianTime / 1000) + "s\n" +
+        "ContinueCompletionProvider - P95 Time: " + (p95Time / 1000) + "s\n" +
+        "ContinueCompletionProvider - P99 Time: " + (p99Time / 1000) + "s\n"
       );
       if (!outcome || !outcome.completion) {
         return null;
diff --git a/extensions/vscode/src/diff/vertical/manager.ts b/extensions/vscode/src/diff/vertical/manager.ts
index 985a6ba29..50949d8ef 100644
--- a/extensions/vscode/src/diff/vertical/manager.ts
+++ b/extensions/vscode/src/diff/vertical/manager.ts
@@ -286,7 +286,6 @@ export class VerticalDiffManager {
     range?: vscode.Range,
   ) {
     vscode.commands.executeCommand("setContext", "continue.diffVisible", true);
-
     let editor = vscode.window.activeTextEditor;
 
     if (!editor) {
@@ -421,7 +420,6 @@ export class VerticalDiffManager {
           onlyOneInsertion,
         ),
       );
-
       // enable a listener for user edits to file while diff is open
       this.enableDocumentChangeListener();
     } catch (e) {
diff --git a/extensions/vscode/src/extension/VsCodeExtension.ts b/extensions/vscode/src/extension/VsCodeExtension.ts
index 78541f8ea..54b347fe8 100644
--- a/extensions/vscode/src/extension/VsCodeExtension.ts
+++ b/extensions/vscode/src/extension/VsCodeExtension.ts
@@ -102,7 +102,7 @@ export class VsCodeExtension {
       ToCoreProtocol,
       FromCoreProtocol
     >();
-
+    
     new VsCodeMessenger(
       inProcessMessenger,
       this.sidebar.webviewProtocol,
@@ -111,7 +111,7 @@ export class VsCodeExtension {
       configHandlerPromise,
       this.workOsAuthProvider,
     );
-
+    
     this.core = new Core(inProcessMessenger, this.ide, async (log: string) => {
       outputChannel.appendLine(
         "==========================================================================",
